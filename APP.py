import streamlit as st
import pandas as pd
import plotly.express as px
import warnings
warnings.filterwarnings("ignore")
from graphviz import Digraph
import tempfile
import os
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import networkx as nx
import pydot
from io import BytesIO

from pm4py.objects.log.obj import EventLog, Trace, Event
from pm4py.objects.log.util import dataframe_utils
from pm4py.algo.discovery.inductive import algorithm as inductive_miner
from pm4py.visualization.process_tree import visualizer as pt_visualizer
from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner
from pm4py.visualization.petri_net import visualizer as pn_visualizer
from pm4py.visualization.petri_net.util import performance_map
import pm4py

from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image
from reportlab.lib.styles import ParagraphStyle
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate
from reportlab.lib.units import inch


# ---------------- UI ----------------
st.set_page_config(page_title="Process Mining (Excel)", layout="wide")
st.title("üß© Process Mining App")
# ---------------- –ê–≤—Ç–æ—Ä—Å—Ç–≤–æ ----------------
st.markdown("---")
st.markdown("¬© 2026 Hanna Nesterenko | [LinkedIn](https://www.linkedin.com/in/anna-nesterenko-bi/)")
st.markdown("---")
st.markdown("–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ Excel-—Ñ–∞–π–ª –∑ –ø–æ–¥—ñ—è–º–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –ø—Ä–æ—Ü–µ—Å—ñ–≤")
st.markdown("–§–∞–π–ª –º–∞—î –º—ñ—Å—Ç–∏—Ç—ñ –æ–±–æ–≤'—è–∑–∫–æ–≤—ñ –ø–æ–ª—è (–∫–æ–∂–µ–Ω —Ä—è–¥–æ–∫ = –ø–æ–¥—ñ—è/–∫—Ä–æ–∫ (event)):")
st.markdown("- case_id ‚Äî —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π –Ω–æ–º–µ—Ä –∞–±–æ –Ω–∞–∑–≤–∞ –∫–µ–π—Å—É")
st.markdown("- activity ‚Äî –Ω–∞–∑–≤–∞ –ø–æ–¥—ñ—ó/–∫—Ä–æ–∫—É")
st.markdown("- timestamp ‚Äî –¥–∞—Ç–∞ –π —á–∞—Å –ø–æ—á–∞—Ç–∫—É –ø–æ–¥—ñ—ó/–∫—Ä–æ–∫—É")

# ---------------- Upload Excel ----------------
uploaded_file = st.file_uploader("–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ Excel –ª–æ–≥", type=["xlsx"])

log = None
df = None

if uploaded_file:
    df = pd.read_excel(uploaded_file)

    required_cols = {"case_id", "activity", "timestamp"}
    if not required_cols.issubset(df.columns):
        st.error("Excel –º–∞—î –º—ñ—Å—Ç–∏—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏: case_id, activity, timestamp")
        st.stop()

    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df = dataframe_utils.convert_timestamp_columns_in_df(df)

    st.success("Excel —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
    st.dataframe(df.head(5))

    # ---------------- Convert to EventLog ----------------
    log = EventLog()

    for case_id, group in df.groupby("case_id"):
        trace = Trace()
        trace.attributes["concept:name"] = str(case_id)

        for _, row in group.sort_values("timestamp").iterrows():
            event = Event()
            event["concept:name"] = row["activity"]
            event["time:timestamp"] = row["timestamp"]
            trace.append(event)

        log.append(trace)

    #st.info(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–µ–π—Å—ñ–≤: {len(log)}")

    
# ---------------- Base analytics ----------------
    st.subheader("üìä –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–æ–≥—ñ–≤")

    # --- –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–µ–π—Å—ñ–≤ ---
    num_cases = df["case_id"].nunique()
    
    # --- –ü–µ—Ä—ñ–æ–¥ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è ---
    start_period = df["timestamp"].min()
    end_period = df["timestamp"].max()
    
    # --- –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∫–µ–π—Å—ñ–≤ ---
    case_times = (
        df.groupby("case_id")["timestamp"]
        .agg(start="min", end="max")
        .reset_index()
    )
    case_times["duration_hours"] = (
        case_times["end"] - case_times["start"]
    ).dt.total_seconds() / 3600
    
    avg_duration = case_times["duration_hours"].mean()
    median_duration = case_times["duration_hours"].median()
    
    # --- –ö—ñ–ª—å–∫—ñ—Å—Ç—å activity –Ω–∞ –∫–µ–π—Å ---
    activities_per_case = (
        df.groupby("case_id")["activity"]
        .count()
    )
    avg_activities = activities_per_case.mean()
    
    # --- –í–∏–≤—ñ–¥ ---
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("–ü–µ—Ä—ñ–æ–¥ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è",
                  f"{start_period.date()} ‚Üí {end_period.date()}")
        st.metric("–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–µ–π—Å—ñ–≤", num_cases)
    
    with col2:
        st.metric("–°–µ—Ä. —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∫–µ–π—Å—É (–≥–æ–¥)",
                  round(avg_duration, 2))
    
        st.metric("–ú–µ–¥—ñ–∞–Ω–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∫–µ–π—Å—É (–≥–æ–¥)",
              round(median_duration, 2))
    
    with col3:        
        st.metric("–°–µ—Ä. –∫—ñ–ª—å–∫—ñ—Å—Ç—å activity –Ω–∞ –∫–µ–π—Å",
              round(avg_activities, 1))

    
    most_common_start = (
        df.sort_values("timestamp")
          .groupby("case_id")
          .head(1)["activity"]
          .value_counts()
          .idxmax()
    )
    
    most_common_end = (
        df.sort_values("timestamp")
          .groupby("case_id")
          .tail(1)["activity"]
          .value_counts()
          .idxmax()
    )

    # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä—ñ–≤ activity –≤ –º–µ–∂–∞—Ö –∫–µ–π—Å—É
    activity_repeats = (
        df.groupby(["case_id", "activity"])
          .size()
          .reset_index(name="count")
    )
    
    # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ —Ç—ñ, —â–æ –ø–æ–≤—Ç–æ—Ä—é–≤–∞–ª–∏—Å—å
    repeated_steps = activity_repeats[
        activity_repeats["count"] > 1
    ]
    
    top_rework = (
        repeated_steps.groupby("activity")["count"]
        .sum()
        .sort_values(ascending=False)
        .head(10)
    )
    
    description = f"""
    –ü—Ä–æ—Ü–µ—Å –∑–∞–∑–≤–∏—á–∞–π –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –∫—Ä–æ–∫—É '{most_common_start}' 
    —Ç–∞ –Ω–∞–π—á–∞—Å—Ç—ñ—à–µ –∑–∞–≤–µ—Ä—à—É—î—Ç—å—Å—è –Ω–∞ –∫—Ä–æ—Ü—ñ '{most_common_end}'.
    
    –°–µ—Ä–µ–¥–Ω—è —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∫–µ–π—Å—É —Å—Ç–∞–Ω–æ–≤–∏—Ç—å {round(avg_duration,2)} –≥–æ–¥–∏–Ω,
    –∞ —Å–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤ –Ω–∞ –∫–µ–π—Å ‚Äî {round(avg_activities,1)}.
    
    –ù–∞–π–±—ñ–ª—å—à–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä—ñ–≤ —Å–ø–æ—Å—Ç–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –Ω–∞ –∫—Ä–æ–∫–∞—Ö:
    {", ".join(top_rework.index.tolist()[:3])}.
    """
    
    st.info(description)
    
    
    case_durations = (
        df.groupby("case_id")["timestamp"]
        .agg(["min", "max"])
        .reset_index()
    )
    case_durations["duration_hours"] = (
        case_durations["max"] - case_durations["min"]
    ).dt.total_seconds() / 3600

    fig = px.histogram(
        case_durations,
        x="duration_hours",
        nbins=20,
        title="–¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∫–µ–π—Å—ñ–≤ (–≥–æ–¥–∏–Ω–∏)"
    )
    st.plotly_chart(fig, use_container_width=True)


# ---------------- last steps ----------------
    st.subheader("üîö –ö—Ä–æ–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –ø—Ä–æ—Ü–µ—Å—É")

    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π –∫—Ä–æ–∫ –∫–æ–∂–Ω–æ–≥–æ –∫–µ–π—Å—É
    last_activities = (
        df.sort_values("timestamp")
          .groupby("case_id")
          .tail(1)["activity"]
    )
    
    top_end_activities = last_activities.value_counts().head(10)
    
    st.write("–¢–û–ü –∫—Ä–æ–∫—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è:")
    st.dataframe(top_end_activities.reset_index()
                 .rename(columns={"index": "activity",
                                  "activity": "–∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–µ–π—Å—ñ–≤"}))


    # ---------------- Rework ----------------
    # –û–±—á–∏—Å–ª–µ–Ω–Ω—è –∫–µ–π—Å—ñ–≤ –∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è–º–∏ 
    activity_counts = (
        df.groupby("case_id")["activity"]
          .value_counts()
          .reset_index(name="count")
    )
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫–µ–π—Å–∏, –¥–µ —è–∫–∞—Å—å –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä—é—î—Ç—å—Å—è –±—ñ–ª—å—à–µ 1 —Ä–∞–∑—É
    cases_with_rework_list = activity_counts.loc[activity_counts["count"] > 1, "case_id"].unique()
    
    st.subheader("üîÅ –ü–æ–≤—Ç–æ—Ä—é–≤–∞–Ω—ñ –∫—Ä–æ–∫–∏ (rework)")
    
    # –¢–û–ü –∫—Ä–æ–∫—ñ–≤ –∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è–º–∏
    top_rework = activity_counts.groupby("activity")["count"].sum().sort_values(ascending=False).head(10)
    st.write("–¢–û–ü –∫—Ä–æ–∫—ñ–≤ –∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è–º–∏:")
    st.dataframe(top_rework.reset_index().rename(columns={"count": "–∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä—ñ–≤"}))
    
    # –ê–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫ –ø–æ –∫–µ–π—Å–∞–º –∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è–º–∏
    total_rework_cases = len(cases_with_rework_list)
    total_cases = df["case_id"].nunique()
    percent_rework = round((total_rework_cases / total_cases) * 100, 2)
    st.markdown(
        f"–í –Ω–∞—à—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ {total_rework_cases} –∫–µ–π—Å—ñ–≤ ({percent_rework}%) –º—ñ—Å—Ç—è—Ç—å –ø–æ–≤—Ç–æ—Ä—é–≤–∞–Ω—ñ –∫—Ä–æ–∫–∏. "
        "–¶–µ –≤–∫–∞–∑—É—î –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å rework —É –ø—Ä–æ—Ü–µ—Å—ñ, —è–∫–∏–π —É–ø–æ–≤—ñ–ª—å–Ω—é—î –π–æ–≥–æ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≤–∏—â—É—î –≤–∞—Ä—ñ–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç—ñ –∫–µ–π—Å—ñ–≤."
    )
    
    # ---------------- –ì—Ä–∞—Ñ—ñ–∫ Lead Time ----------------
    st.markdown("### üìà –†–æ–∑–ø–æ–¥—ñ–ª Lead Time: –∫–µ–π—Å–∏ –∑ rework vs –±–µ–∑")
    
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    
    # –ì—Ä—É–ø—É—î–º–æ –ø–æ –∫–µ–π—Å—É —ñ –æ–±—á–∏—Å–ª—é—î–º–æ Lead Time (–≥–æ–¥–∏–Ω–∏)
    lead_time_per_case = (
        df.groupby("case_id")["timestamp"]
          .agg(lead_time=lambda x: (x.max() - x.min()).total_seconds() / 3600)
          .reset_index()
    )
    
    # –î–æ–¥–∞—î–º–æ –∫–æ–ª–æ–Ω–∫—É Rework
    lead_time_per_case["rework"] = lead_time_per_case["case_id"].isin(cases_with_rework_list)
    lead_time_per_case["rework_label"] = lead_time_per_case["rework"].map({True: "–ó –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è–º–∏", False: "–ë–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω—å"})
    
    # –§—ñ–≥—É—Ä–∞
    plt.figure(figsize=(5,3))
    
    # –ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∞ + Boxplot
    #sns.histplot(
        #data=lead_time_per_case,
        #x="lead_time",
        #hue="rework_label",
        #bins=20,
        #kde=True,
        #palette={"–ó –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è–º–∏": "red", "–ë–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω—å": "green"},
        #alpha=0.6
    #)
    
    sns.boxplot(
        data=lead_time_per_case,
        x="lead_time",
        y="rework_label",
        palette={"–ó –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è–º–∏": "red", "–ë–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω—å": "green"},
        width=0.3,
        fliersize=3
    )
    
    plt.xlabel("Lead Time (–≥–æ–¥)")
    plt.ylabel("")
    plt.title("–†–æ–∑–ø–æ–¥—ñ–ª —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç—ñ –∫–µ–π—Å—ñ–≤ –∑ Rework —Ç–∞ –±–µ–∑")
    plt.tight_layout()
    st.pyplot(plt.gcf())
    
    # ---------------- –°–µ—Ä–µ–¥–Ω—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏ ----------------

    # –ü—Ä–∏–ø—É—Å—Ç–∏–º–æ, Waiting Time = Lead Time –º—ñ–Ω—É—Å —Å—É–º—É —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π
    # –°–ø—Ä–æ—â–µ–Ω–æ, —è–∫—â–æ —É –Ω–µ–º–∞—î —Ñ–∞–∫—Ç–∏—á–Ω–æ—ó —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç—ñ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π, –º–æ–∂–Ω–∞ –ø—Ä–æ—Å—Ç–æ —è–∫ –¥–µ–ª—å—Ç—É –º—ñ–∂ –∫—Ä–æ–∫–∞–º–∏:
    
    waiting_time_per_case = (
        df.groupby("case_id")["timestamp"]
          .agg(waiting_time=lambda x: ((x.max() - x.min()).total_seconds() / 3600) * 0.3)  # –Ω–∞–ø—Ä–∏–∫–ª–∞–¥ 30% Lead Time
          .reset_index()
    )
    waiting_time_per_case.rename(columns={"waiting_time": "waiting_time_hrs"}, inplace=True)

    mean_lead_rework = lead_time_per_case.loc[lead_time_per_case["rework"], "lead_time"].mean()
    mean_lead_no_rework = lead_time_per_case.loc[~lead_time_per_case["rework"], "lead_time"].mean()
    
    # –Ø–∫—â–æ waiting_time_per_case —ñ—Å–Ω—É—î –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–æ
    if "waiting_time_hrs" in waiting_time_per_case.columns:
        mean_wait_rework = waiting_time_per_case.loc[lead_time_per_case["rework"], "waiting_time_hrs"].mean()
        mean_wait_no_rework = waiting_time_per_case.loc[~lead_time_per_case["rework"], "waiting_time_hrs"].mean()
    else:
        mean_wait_rework = mean_wait_no_rework = 0
    
    st.markdown(
        f"**–°–µ—Ä–µ–¥–Ω—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏ –ø–æ –≥—Ä—É–ø–∞—Ö:**\n\n"
        f"- –ö–µ–π—Å–∏ –∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è–º–∏: Lead Time = {mean_lead_rework:.2f} –≥–æ–¥, Waiting Time = {mean_wait_rework:.2f} –≥–æ–¥\n"
        f"- –ö–µ–π—Å–∏ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω—å: Lead Time = {mean_lead_no_rework:.2f} –≥–æ–¥, Waiting Time = {mean_wait_no_rework:.2f} –≥–æ–¥"
    )

  
# ---------------- Heuristics Miner ----------------
if log is not None:
    st.subheader("Heuristics Miner")
    st.markdown("Heuristics Miner ‚Üí Petri Net –ø–æ–∫–∞–∑—É—î —Ä–µ–∞–ª—å–Ω–∏–π, —á–∞—Å—Ç–æ—Ç–Ω–∏–π –ø—Ä–æ—Ü–µ—Å")
    st.markdown("–¶–µ –≥—Ä–∞—Ñ –ø–µ—Ä–µ—Ö–æ–¥—ñ–≤, –±–ª–∏–∂—á–∏–π –¥–æ ¬´—è–∫ —Ä–µ–∞–ª—å–Ω–æ –≤—ñ–¥–±—É–≤–∞–ª–æ—Å—è¬ª")
    st.markdown(" ")
    st.markdown("–û—Å–Ω–æ–≤–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ Petri Net:")
    st.markdown("- ‚óØ –ö—Ä—É–∂–∫–∏ (places). –°—Ç–∞–Ω–∏ –ø—Ä–æ—Ü–µ—Å—É ¬´–¢—É—Ç –º–∏ –∑–∞—Ä–∞–∑¬ª")
    st.markdown("- ‚ñ≠ –ü—Ä—è–º–æ–∫—É—Ç–Ω–∏–∫–∏ (transitions). –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ, –†–µ–∞–ª—å–Ω—ñ –¥—ñ—ó")
    st.markdown("- ‚ûù –°—Ç—Ä—ñ–ª–∫–∏. –ü–æ—Ç—ñ–∫ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è")
    st.markdown(" ")
    st.markdown("–ß–∞—Å—Ç–æ—Ç–∏ / —Ç–æ–≤—â–∏–Ω–∞ —Å—Ç—Ä—ñ–ª–æ–∫")
    st.markdown("üìå –ß–∏—Ç–∞—î—Ç—å—Å—è:")
    st.markdown("—Ç–æ–≤—Å—Ç—ñ —Å—Ç—Ä—ñ–ª–∫–∏ ‚Üí —á–∞—Å—Ç–æ")
    st.markdown("—Ç–æ–Ω–∫—ñ ‚Üí —Ä—ñ–¥–∫–æ")
    st.markdown("–¶–µ –¥—É–∂–µ –≤–∞–∂–ª–∏–≤–æ –¥–ª—è: bottleneck analysis, –≤—ñ–¥—Ö–∏–ª–µ–Ω—å")
    st.markdown(" ")
    st.markdown("üß† –Ø–∫ —á–∏—Ç–∞—Ç–∏ Heuristics Miner –ø—Ä–∞–∫—Ç–∏—á–Ω–æ")
    st.markdown("1. –ó–Ω–∞–π–¥–∏ Start ‚Üí End")
    st.markdown("2. –ü–æ–¥–∏–≤–∏—Å—å: –¥–µ –Ω–∞–π–±—ñ–ª—å—à–µ –≥—ñ–ª–æ–∫, –¥–µ —î –∑–≤–æ—Ä–æ—Ç–Ω—ñ —Å—Ç—Ä—ñ–ª–∫–∏")
    st.markdown("3. –®—É–∫–∞–π: loops (–ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è –Ω–∞–∑–∞–¥), –æ–±—Ö–æ–¥–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–∞—Ä—à—Ä—É—Ç—É")
    st.markdown("4. –ó–∞–¥–∞–π –ø–∏—Ç–∞–Ω–Ω—è: –ß–æ–º—É —Ç—É—Ç —Ç–∞–∫ –±–∞–≥–∞—Ç–æ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤? –ß–æ–º—É —Ç—É—Ç –ø–æ–≤–µ—Ä—Ç–∞—é—Ç—å—Å—è –Ω–∞–∑–∞–¥?")
    st.markdown(" ")
    st.markdown("üìå Heuristics Miner = —Ä–µ–∞–ª—å–Ω–∞ –ø–æ–≤–µ–¥—ñ–Ω–∫–∞, –∑ —à—É–º–æ–º")
    st.markdown(" ")
 

    # –ü–æ–¥—ñ—ó –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω—ñ
    df_sorted = df.sort_values(["case_id", "timestamp"])
    
    # –ù–∞—Å—Ç—É–ø–Ω–∞ activity —Ç–∞ timestamp
    df_sorted["next_activity"] = (
        df_sorted.groupby("case_id")["activity"].shift(-1)
    )
    
    df_sorted["next_timestamp"] = (
        df_sorted.groupby("case_id")["timestamp"].shift(-1)
    )
    
    # –ß–∞—Å –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è
    df_sorted["waiting_time_hours"] = (
        df_sorted["next_timestamp"] - df_sorted["timestamp"]
    ).dt.total_seconds() / 3600
    
    # –í–∏–¥–∞–ª—è—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ –ø–æ–¥—ñ—ó –∫–µ–π—Å—ñ–≤
    transitions = df_sorted.dropna(subset=["next_activity"])
    
    # –ê–≥—Ä–µ–≥–∞—Ü—ñ—è
    edges = (
        transitions
        .groupby(["activity", "next_activity"])
        .agg(
            frequency=("case_id", "count"),
            avg_waiting=("waiting_time_hours", "mean")
        )
        .reset_index()
    )

    # Bottleneck = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π avg_waiting —Å–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ö–æ–¥—ñ–≤
    bottleneck_row = edges.loc[edges["avg_waiting"].idxmax()]
    
    bottleneck_text = (
        f"–ù–∞–π–±—ñ–ª—å—à–∏–π bottleneck: "
        f"{bottleneck_row['activity']} ‚Üí {bottleneck_row['next_activity']} "
        f"(—Å–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å: {bottleneck_row['avg_waiting']:.2f} –≥–æ–¥, "
        f"—á–∞—Å—Ç–æ—Ç–∞: {bottleneck_row['frequency']})"
    )

    # –¢–æ–≤—â–∏–Ω–∞ —Å—Ç—Ä—ñ–ª–æ–∫
    edges["penwidth"] = (
        edges["frequency"] / edges["frequency"].max() * 5
    ).clip(lower=1)
    
    # –ö–æ–ª—ñ—Ä –∑–∞ waiting time
    def waiting_to_color(hours):
        if hours < 1:
            return "green"
        elif hours < 4:
            return "orange"
        else:
            return "red"
    
    edges["color"] = edges["avg_waiting"].apply(waiting_to_color)
    
    st.subheader("üî• Heuristics Miner (Custom Graphviz)")
    
    dot = Digraph(
        engine="dot",
        graph_attr={"rankdir": "LR"},
        node_attr={"shape": "box", "style": "rounded,filled", "fillcolor": "#F9F9F9"}
    )

    # --- –õ–ï–ì–ï–ù–î–ê ---
    with dot.subgraph(name="cluster_legend") as c:
        c.attr(label="Legend", fontsize="12")
        c.node("L1", "üü¢ < 1 –≥–æ–¥", shape="box", style="filled", fillcolor="green")
        c.node("L2", "üü† 1‚Äì4 –≥–æ–¥", shape="box", style="filled", fillcolor="orange")
        c.node("L3", "üî¥ > 4 –≥–æ–¥", shape="box", style="filled", fillcolor="red")
    
    # –î–æ–¥–∞—î–º–æ –≤—Å—ñ activity —è–∫ –≤—É–∑–ª–∏
    activities = set(edges["activity"]).union(edges["next_activity"])
    for act in activities:
        dot.node(act)
    
    # –î–æ–¥–∞—î–º–æ —Ä–µ–±—Ä–∞ –∑ –∫–∞—Å—Ç–æ–º–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    for _, row in edges.iterrows():
        dot.edge(
            row["activity"],
            row["next_activity"],
            label=f'{row["frequency"]} | {row["avg_waiting"]:.1f}h',
            penwidth=str(row["penwidth"]),
            color=row["color"]
        )

    #–¢–µ–∫—Å—Ç–æ–≤–∏–π –æ–ø–∏—Å bottleneck –ø—Ä—è–º–æ —É –≥—Ä–∞—Ñ—ñ
    dot.node(
        "bottleneck_info",
        bottleneck_text,
        shape="note",
        style="filled",
        fillcolor="#FFE4E1"
    )

    
    st.graphviz_chart(dot)

    st.markdown(" ")
    st.markdown("–Ø–∫ —Ü–µ —á–∏—Ç–∞—Ç–∏ (–ø—Ä–∞–∫—Ç–∏—á–Ω–æ):")
    st.markdown(" ")
    st.markdown("üî¥ —Ç–æ–≤—Å—Ç–∞ + —á–µ—Ä–≤–æ–Ω–∞ ‚Üí –∫—Ä–∏—Ç–∏—á–Ω–∏–π bottleneck")
    st.markdown("üü¢ —Ç–æ–≤—Å—Ç–∞ + –∑–µ–ª–µ–Ω–∞ ‚Üí —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π —à–ª—è—Ö")
    st.markdown(" ")

    
    
    st.markdown(" ")
    st.markdown(" ")


    # ---------------- Variant analysis ----------------
    st.subheader("‚ö° Variant analysis (–¢–û–ü 5 —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤)")
    
    # –§–æ—Ä–º—É—î–º–æ —à–ª—è—Ö –∫–µ–π—Å—É
    variants = (
        df.sort_values("timestamp")
          .groupby("case_id")["activity"]
          .apply(lambda x: " ‚Üí ".join(x))
    )
    
    # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_cases = variants.count()
    unique_variants = variants.nunique()
    
    variant_counts_full = variants.value_counts()
    variant_counts_top5 = variant_counts_full.head(5)
    
    # –¢–∞–±–ª–∏—Ü—è –¢–û–ü 5
    variant_counts = (
        variant_counts_top5
        .reset_index()
    )
    variant_counts.columns = ["–°—Ü–µ–Ω–∞—Ä—ñ–π –ø—Ä–æ—Ü–µ—Å—É", "–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–µ–π—Å—ñ–≤"]
    
    st.dataframe(variant_counts)
    
    # ---------------- –î–æ–¥–∞—Ç–∫–æ–≤–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞ ----------------
    
    top1_share = variant_counts_full.iloc[0] / total_cases * 100
    top5_share = variant_counts_top5.sum() / total_cases * 100
    
    st.markdown("### üìä –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤")
    
    st.write(f"üî¢ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–µ–π—Å—ñ–≤: **{total_cases}**")
    st.write(f"üß≠ –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤: **{unique_variants}**")
    st.write(f"ü•á –ß–∞—Å—Ç–∫–∞ –Ω–∞–π–ø–æ—à–∏—Ä–µ–Ω—ñ—à–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä—ñ—é: **{top1_share:.1f}%**")
    st.write(f"üèÜ –ß–∞—Å—Ç–∫–∞ –¢–û–ü-5 —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤: **{top5_share:.1f}%**")
    
    # ---------------- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫ ----------------
    
        
    if unique_variants == 1:
        conclusion = "–ü—Ä–æ—Ü–µ—Å –ø–æ–≤–Ω—ñ—Å—Ç—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–∏–π. –í—Å—ñ –∫–µ–π—Å–∏ –ø—Ä–æ—Ö–æ–¥—è—Ç—å –æ–¥–Ω–∞–∫–æ–≤–∏–π —Å—Ü–µ–Ω–∞—Ä—ñ–π."
    
    elif top1_share > 70:
        conclusion = (
            "–ü—Ä–æ—Ü–µ—Å –ø–µ—Ä–µ–≤–∞–∂–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–∏–π. "
            "–ë—ñ–ª—å—à—ñ—Å—Ç—å –∫–µ–π—Å—ñ–≤ —Å–ª—ñ–¥—É—é—Ç—å –æ–¥–Ω–æ–º—É –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å—Ü–µ–Ω–∞—Ä—ñ—é."
        )
    
    elif top5_share > 70:
        conclusion = (
            "–ü—Ä–æ—Ü–µ—Å –º–∞—î –ø–æ–º—ñ—Ä–Ω—É –≤–∞—Ä—ñ–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å. "
            "–Ü—Å–Ω—É—î –∫—ñ–ª—å–∫–∞ –¥–æ–º—ñ–Ω—É—é—á–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤."
        )
    
    else:
        conclusion = (
            "–ü—Ä–æ—Ü–µ—Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—î—Ç—å—Å—è –≤–∏—Å–æ–∫–æ—é –≤–∞—Ä—ñ–∞–±–µ–ª—å–Ω—ñ—Å—Ç—é. "
            "–í–µ–ª–∏–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤ –º–æ–∂–µ —Å–≤—ñ–¥—á–∏—Ç–∏ "
            "–ø—Ä–æ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω—ñ –ø—Ä–æ—Ü–µ–¥—É—Ä–∏ –∞–±–æ –≤–∏–Ω—è—Ç–∫–æ–≤—ñ –∫–µ–π—Å–∏."
        )
    
    st.info(conclusion)

    
    
# ---------------- Timeline –∫–µ–π—Å—É ----------------
    st.subheader("üìÖ Timeline –∫–µ–π—Å—É")
    
    case_list = df["case_id"].unique()
    selected_case = st.selectbox("–û–±–µ—Ä—ñ—Ç—å –∫–µ–π—Å", case_list)
    
    case_df = df[df["case_id"] == selected_case] \
        .sort_values("timestamp")
    
    fig = px.scatter(
        case_df,
        x="timestamp",
        y="activity",
        title=f"Timeline –∫–µ–π—Å—É {selected_case}",
    )
    
    st.plotly_chart(fig, use_container_width=True)

    


